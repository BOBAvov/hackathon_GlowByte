import os
import io
import pandas as pd
import numpy as np
import xgboost as xgb
from contextlib import asynccontextmanager
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
import uvicorn

# Ð“Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ
MODEL_PATH = "coal_fire_model.json"
bst_model = None


# --- LIFESPAN ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    global bst_model
    print("ðŸš€ Ð—Ð°Ð¿ÑƒÑÐº Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ...")

    if os.path.exists(MODEL_PATH):
        try:
            bst_model = xgb.XGBClassifier()
            bst_model.load_model(MODEL_PATH)
            print(f"âœ… ÐœÐ¾Ð´ÐµÐ»ÑŒ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð° Ð¸Ð· {MODEL_PATH}")
        except Exception as e:
            print(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐµ Ð¼Ð¾Ð´ÐµÐ»Ð¸: {e}")
    else:
        print(f"âš ï¸ Ð¤Ð°Ð¹Ð» Ð¼Ð¾Ð´ÐµÐ»Ð¸ {MODEL_PATH} Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½!")

    yield
    print("ðŸ›‘ ÐžÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ...")


app = FastAPI(title="Coal Fire Prediction API", version="2.3", lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# --- Ð›ÐžÐ“Ð˜ÐšÐ ÐžÐ‘Ð ÐÐ‘ÐžÐ¢ÐšÐ˜ (ETL) ---

def process_data_pipeline(weather_content, supplies_content, temperature_content):
    try:
        # 1. Ð§Ñ‚ÐµÐ½Ð¸Ðµ
        weather = pd.read_csv(io.BytesIO(weather_content))
        supplies = pd.read_csv(io.BytesIO(supplies_content))
        temperature = pd.read_csv(io.BytesIO(temperature_content))

        # 2. ÐŸÑ€ÐµÐ´Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ°
        weather["date"] = pd.to_datetime(weather["date"], errors='coerce')
        weather["date_norm"] = weather["date"].dt.normalize()

        supplies["start_date"] = pd.to_datetime(supplies["Ð’Ñ‹Ð³Ñ€ÑƒÐ·ÐºÐ°ÐÐ°Ð¡ÐºÐ»Ð°Ð´"], errors='coerce')
        supplies["end_date"] = pd.to_datetime(supplies["ÐŸÐ¾Ð³Ñ€ÑƒÐ·ÐºÐ°ÐÐ°Ð¡ÑƒÐ´Ð½Ð¾"], errors='coerce')

        temperature["date"] = pd.to_datetime(temperature["Ð”Ð°Ñ‚Ð° Ð°ÐºÑ‚Ð°"], errors='coerce')
        if 'ÐœÐ°Ñ€ÐºÐ°' in temperature.columns:
            temperature['ÐœÐ°Ñ€ÐºÐ°'] = temperature['ÐœÐ°Ñ€ÐºÐ°'].astype(str).apply(lambda x: x.split('-')[0])

        # 3. Ð Ð°Ð·Ð²Ð¾Ñ€Ð°Ñ‡Ð¸Ð²Ð°Ð½Ð¸Ðµ Supplies
        grouped_supplies = supplies.groupby(['Ð¡ÐºÐ»Ð°Ð´', 'Ð¨Ñ‚Ð°Ð±ÐµÐ»ÑŒ']).agg(
            start_min=('start_date', 'min'),
            end_max=('end_date', 'max')
        ).reset_index()

        res_supplies = pd.DataFrame()

        for i, row in grouped_supplies.iterrows():
            if pd.isna(row['start_min']) or pd.isna(row['end_max']):
                continue

            dates_range = pd.date_range(start=row['start_min'], end=row['end_max'], freq='D')
            subset_supplies = supplies[(supplies['Ð¡ÐºÐ»Ð°Ð´'] == row['Ð¡ÐºÐ»Ð°Ð´']) & (supplies['Ð¨Ñ‚Ð°Ð±ÐµÐ»ÑŒ'] == row['Ð¨Ñ‚Ð°Ð±ÐµÐ»ÑŒ'])]
            unique_types = subset_supplies['ÐÐ°Ð¸Ð¼. Ð•Ð¢Ð¡ÐÐ“'].unique()

            for type_col in unique_types:
                local_df = pd.DataFrame({'date': dates_range})
                local_df['Ð¡ÐºÐ»Ð°Ð´'] = row['Ð¡ÐºÐ»Ð°Ð´']
                local_df['Ð¨Ñ‚Ð°Ð±ÐµÐ»ÑŒ'] = row['Ð¨Ñ‚Ð°Ð±ÐµÐ»ÑŒ']
                local_df['ÐÐ°Ð¸Ð¼. Ð•Ð¢Ð¡ÐÐ“'] = type_col

                temp = subset_supplies[subset_supplies['ÐÐ°Ð¸Ð¼. Ð•Ð¢Ð¡ÐÐ“'] == type_col]
                if temp.empty:
                    local_df['ÐœÐ°ÑÑÐ° ÑƒÐ³Ð»Ñ'] = 0.0
                else:
                    idx = pd.to_datetime(dates_range).normalize()
                    arrivals = temp.groupby(temp['start_date'].dt.normalize())['ÐÐ° ÑÐºÐ»Ð°Ð´, Ñ‚Ð½'].sum()
                    shipments = temp.groupby(temp['end_date'].dt.normalize())['ÐÐ° ÑÑƒÐ´Ð½Ð¾, Ñ‚Ð½'].sum()
                    stock = (arrivals.reindex(idx, fill_value=0).cumsum() -
                             shipments.reindex(idx, fill_value=0).cumsum()).clip(lower=0)
                    local_df['ÐœÐ°ÑÑÐ° ÑƒÐ³Ð»Ñ'] = local_df['date'].dt.normalize().map(stock).fillna(0).astype(float)

                res_supplies = pd.concat([res_supplies, local_df], ignore_index=True)

        if res_supplies.empty:
            return {"status": "error", "message": "Empty supplies data"}

        # 4. ÐœÐµÑ€Ð´Ð¶ Ð´Ð°Ð½Ð½Ñ‹Ñ…
        res_supplies['date'] = pd.to_datetime(res_supplies['date'])
        res_supplies['date_norm'] = res_supplies['date'].dt.normalize()
        weather_agg = weather.groupby('date_norm').mean(numeric_only=True).reset_index()
        full_df = res_supplies.merge(weather_agg, on='date_norm', how='left')

        # ÐœÐµÑ€Ð´Ð¶ Ñ Ñ‚ÐµÐ¼Ð¿ÐµÑ€Ð°Ñ‚ÑƒÑ€Ð¾Ð¹
        temp_subset = temperature[['date', 'Ð¡ÐºÐ»Ð°Ð´', 'Ð¨Ñ‚Ð°Ð±ÐµÐ»ÑŒ', 'ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ Ñ‚ÐµÐ¼Ð¿ÐµÑ€Ð°Ñ‚ÑƒÑ€Ð°', 'ÐœÐ°Ñ€ÐºÐ°']].copy()
        full_df['ÐÐ°Ð¸Ð¼. Ð•Ð¢Ð¡ÐÐ“'] = full_df['ÐÐ°Ð¸Ð¼. Ð•Ð¢Ð¡ÐÐ“'].astype(str)
        temp_subset['ÐœÐ°Ñ€ÐºÐ°'] = temp_subset['ÐœÐ°Ñ€ÐºÐ°'].astype(str)

        full_df = full_df.merge(
            temp_subset,
            left_on=['date', 'Ð¡ÐºÐ»Ð°Ð´', 'Ð¨Ñ‚Ð°Ð±ÐµÐ»ÑŒ', 'ÐÐ°Ð¸Ð¼. Ð•Ð¢Ð¡ÐÐ“'],
            right_on=['date', 'Ð¡ÐºÐ»Ð°Ð´', 'Ð¨Ñ‚Ð°Ð±ÐµÐ»ÑŒ', 'ÐœÐ°Ñ€ÐºÐ°'],
            how='left'
        )

        if 'ÐœÐ°Ñ€ÐºÐ°' in full_df.columns:
            full_df.drop(columns=['ÐœÐ°Ñ€ÐºÐ°'], inplace=True)

        # [FIX 1] Ð—Ð°Ð¼ÐµÐ½Ð° interpolate Ð½Ð° ffill
        full_df['ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ Ñ‚ÐµÐ¼Ð¿ÐµÑ€Ð°Ñ‚ÑƒÑ€Ð°'] = full_df.groupby(['Ð¡ÐºÐ»Ð°Ð´', 'Ð¨Ñ‚Ð°Ð±ÐµÐ»ÑŒ'])[
            'ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ Ñ‚ÐµÐ¼Ð¿ÐµÑ€Ð°Ñ‚ÑƒÑ€Ð°'].transform(lambda x: x.ffill().fillna(0))

        if 'visibility' in full_df.columns:
            full_df.drop(columns=['visibility'], inplace=True)

        # 5. Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²
        df_feat = full_df.sort_values(by=['Ð¡ÐºÐ»Ð°Ð´', 'Ð¨Ñ‚Ð°Ð±ÐµÐ»ÑŒ', 'date'])
        base_features = ['t', 'humidity', 'wind_dir', 'v_avg', 'ÐœÐ°ÑÑÐ° ÑƒÐ³Ð»Ñ', 'ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ Ñ‚ÐµÐ¼Ð¿ÐµÑ€Ð°Ñ‚ÑƒÑ€Ð°']
        available_features = [f for f in base_features if f in df_feat.columns]

        for col in available_features:
            if col == 'ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ Ñ‚ÐµÐ¼Ð¿ÐµÑ€Ð°Ñ‚ÑƒÑ€Ð°':
                continue
            for lag in [1, 3, 7]:
                df_feat[f'{col}_lag_{lag}'] = df_feat.groupby(['Ð¡ÐºÐ»Ð°Ð´', 'Ð¨Ñ‚Ð°Ð±ÐµÐ»ÑŒ'])[col].shift(lag)

        for col in ['t', 'ÐœÐ°ÑÑÐ° ÑƒÐ³Ð»Ñ']:
            if col in df_feat.columns:
                df_feat[f'{col}_ma_7'] = df_feat.groupby(['Ð¡ÐºÐ»Ð°Ð´', 'Ð¨Ñ‚Ð°Ð±ÐµÐ»ÑŒ'])[col].transform(
                    lambda x: x.rolling(7).mean())


        numeric_cols = df_feat.select_dtypes(include=[np.number]).columns
        df_feat[numeric_cols] = df_feat[numeric_cols].ffill().fillna(0)

        df_feat = df_feat[df_feat['ÐœÐ°ÑÑÐ° ÑƒÐ³Ð»Ñ']>0]
        df_feat = df_feat.drop_duplicates(subset=['Ð¡ÐºÐ»Ð°Ð´','Ð¨Ñ‚Ð°Ð±ÐµÐ»ÑŒ'], keep='last')


        # 6. ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ðµ
        if bst_model and not df_feat.empty:
            booster = bst_model.get_booster()
            expected_features = booster.feature_names

            missing_cols = [c for c in expected_features if c not in df_feat.columns]
            if missing_cols:
                for c in missing_cols:
                    df_feat[c] = 0

            X_pred = df_feat[expected_features]
            probs = bst_model.predict_proba(X_pred)[:, 1]
            df_feat['fire_probability'] = probs
            df_feat['is_dangerous'] = (probs > 0.5).astype(int)
        else:
            df_feat['fire_probability'] = 0.0
            df_feat['is_dangerous'] = 0

        # 7. Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚
        df_feat = df_feat.replace([np.inf, -np.inf], 0).fillna(0)

        # [FIX 2] Ð”Ð¾Ð±Ð°Ð²Ð¸Ð»Ð¸ .copy() Ñ‡Ñ‚Ð¾Ð±Ñ‹ ÑƒÐ±Ñ€Ð°Ñ‚ÑŒ Warning
        result_df = df_feat[['date', 'Ð¡ÐºÐ»Ð°Ð´', 'Ð¨Ñ‚Ð°Ð±ÐµÐ»ÑŒ', 'ÐœÐ°ÑÑÐ° ÑƒÐ³Ð»Ñ', 'fire_probability', 'is_dangerous']].copy()
        result_df['date'] = result_df['date'].dt.strftime('%Y-%m-%d')

        top_risks = result_df[result_df['fire_probability'] > 0.1].sort_values('fire_probability',
                                                                               ascending=False).head(200)

        return {
            "status": "success",
            "total_records": len(result_df),
            "high_risk_count": int(result_df['is_dangerous'].sum()),
            "top_risks": top_risks.to_dict(orient='records'),
            "warehouse_stats": result_df.groupby('Ð¡ÐºÐ»Ð°Ð´')['is_dangerous'].sum().to_dict()
        }

    except Exception as e:
        print(f"Pipeline Error: {e}")
        import traceback
        traceback.print_exc()
        return {"status": "error", "message": str(e)}


@app.post("/predict-fire-risk")
async def predict_fire_risk(
        weather_file: UploadFile = File(...),
        supplies_file: UploadFile = File(...),
        temperature_file: UploadFile = File(...)
):
    w_content = await weather_file.read()
    s_content = await supplies_file.read()
    t_content = await temperature_file.read()

    result = process_data_pipeline(w_content, s_content, t_content)

    if result["status"] == "error":
        raise HTTPException(status_code=500, detail=result["message"])
    return result


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8080)